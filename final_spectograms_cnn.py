# -*- coding: utf-8 -*-
"""final_spectograms_cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1swd1RZ5LOP-fJfUZNfANNjzSNNQ5i5aw
"""

import pandas as pd
import scipy.signal
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL as pil
import numpy as np
import cv2
import sys
from keras.layers import Flatten
from keras.layers import Dense
from keras.layers import Dropout
from keras import optimizers
from tensorflow import keras
import PIL as pil
import multiprocessing as mp
import pickle
from keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.applications.vgg16 import VGG16
import tensorflow as tf

from google.colab import drive
drive.mount('/content/drive')

class df_holder:
    def __init__(self, df): 
        self.df = df

lead_data = pd.read_pickle("/content/drive/MyDrive/ML_Project_Amo/class_dataset.pkl")

train_1 = lead_data.loc[:, ['V1', 'V1_df', 'V2', 'V2_df']]
test = lead_data.loc[:, ['V3', 'V3_df']]
a_df = pd.concat([train_1['V1'], train_1['V2']])
b_df = pd.concat([train_1['V1_df'], train_1['V2_df']])

# create new dataframe with concatenated columns
train = pd.DataFrame({'target': a_df.values, 'df_holder': b_df.values})
train
test = test.rename(columns={'V3': 'target', 'V3_df': 'df_holder'})
test
X_train_, y_train = train['df_holder'].values, train['target'].values
X_test_, y_test = test['df_holder'].values, test['target'].values

import numpy as np
X_test = np.zeros((202, 5000, 12))
X_train = np.zeros((404, 5000, 12))
print(X_test.shape)
for i in range(len(X_test)):
    X_test[i] = X_test_[i].df[1:5001].values.astype(float)
for i in range(len(X_train)):
    X_train[i] = X_train_[i].df[1:5001].values.astype(float)

print(train['target'].value_counts())

# Separating classes
from sklearn.utils import resample
burntout = train[train['target'].values == 1]
not_burntout = train[train['target'].values == 0]

undersample = resample(burntout, 
                       replace=True, 
                       n_samples=len(not_burntout), #set the number of samples to equal the number of the minority class
                       random_state=42)
# Returning to new training set
train_balanced = pd.concat([not_burntout, undersample])

print(train_balanced['target'].value_counts())

X_train_, y_train = train_balanced['df_holder'].values, train_balanced['target'].values

import numpy as np
X_test = np.zeros((202, 5000, 12))
X_train = np.zeros((404, 5000, 12))
print(X_test.shape)
for i in range(len(X_test)):
    X_test[i] = X_test_[i].df[1:5001].values.astype(float)
for i in range(len(X_train)):
    X_train[i] = X_train_[i].df[1:5001].values.astype(float)

"""# don't run from here"""

os.mkdir('drive/MyDrive/Spectrograms/train')
for j in range(len(X_train)):
  os.mkdir(f'drive/MyDrive/Spectrograms/train/Participant{j+1}')
  for i in range(12):
    data = pd.DataFrame(X_train[j])
    data = data.iloc[:,i]
    # plt.figure()
    plt.specgram(data)
    # plt.show()
    plt.savefig(f'drive/MyDrive/Spectrograms/train/Participant{j+1}/lead{i+1}.png')

os.mkdir('drive/MyDrive/Spectrograms/test')
for j in range(len(X_test)):
  os.mkdir(f'drive/MyDrive/Spectrograms/test/Participant{j+1}')
  for i in range(12):
    data = pd.DataFrame(X_train[j])
    data = data.iloc[:,i]
    # plt.figure()
    plt.specgram(data)
    # plt.show()
    plt.savefig(f'drive/MyDrive/Spectrograms/test/Participant{j+1}/lead{i+1}.png')

"""## run from here"""

train_path = os.listdir("/content/drive/MyDrive/Spectrograms/train")
test_path = os.listdir("/content/drive/MyDrive/Spectrograms/test")

def process_image(path):
  img = pil.Image.open(path)
  img = img.resize((224,224))
  img = img.convert('RGB')
  data = np.asarray(img)
  return data

train_data = []
for i in train_path:
  print(i)
  participant_path = os.listdir("/content/drive/MyDrive/Spectrograms/train/"+i)
  input_vector = []
  with mp.Pool(processes=8) as pool:
    input_paths = [f"/content/drive/MyDrive/Spectrograms/train/{i}/{j}" for j in participant_path]
    input_vector = pool.map(process_image, input_paths)
  array = np.array(input_vector)
  train_data.append(array)

train_data = np.array(train_data)

test_data = []
for i in test_path:
  print(i)
  participant_path = os.listdir("/content/drive/MyDrive/Spectrograms/test/"+i)
  input_vector = []
  with mp.Pool(processes=8) as pool:
    input_paths = [f"/content/drive/MyDrive/Spectrograms/test/{i}/{j}" for j in participant_path]
    input_vector = pool.map(process_image, input_paths)
  array = np.array(input_vector)
  test_data.append(array)

test_data = np.array(test_data)

file_name_test = "/content/drive/MyDrive/Spectrograms/test_set"
file_name_train = "/content/drive/MyDrive/Spectrograms/train_set"

train_set = train_data

with open(file_name_train, 'wb') as f:
  pickle.dump(train_data,f)

test_set = test_data

with open(file_name_test, 'wb') as f:
  pickle.dump(test_data,f)

new_y_train = []
for i in range(len(y_train)):
  temp = [y_train[i]]*12
  new_y_train.append(temp)

y_train = np.array(new_y_train)

len(y_train)
y_train = y_train.flatten()
len(y_train)





new_y_test = []
for i in range(len(y_test)):
  temp = [y_test[i]]*12
  new_y_test.append(temp)

y_test = np.array(new_y_test)

with open('/content/drive/MyDrive/Spectrograms/train_set', 'rb') as f:
    train_data = pickle.load(f)

with open('/content/drive/MyDrive/Spectrograms/test_set', 'rb') as f:
    test_data = pickle.load(f)

X_train = []

for i in range(len(train_data)):
  for j in range(len(train_data[0])):
    X_train.append(train_data[i][j])

len(X_train)
X_train = np.array(X_train)
# X_test, y_test = test_set

vgg_model = VGG16(weights='imagenet',include_top=False, input_shape=(224, 224, 3))

input_shape = (12, 224, 224, 3)
input_tensor = Input(shape=input_shape)

# Reshape input tensor to have the correct shape
# x = tf.reshape(input_tensor, [-1, input_shape[1], input_shape[2], input_shape[3]])


input_tensor = Input(shape=(224, 224, 3))
vgg_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)

# Add your own layers on top of the VGG16 model here
# ...

# model = Model(inputs=input_tensor, outputs=...)

for layer in vgg_model.layers:
  layer.trainable = False
# Make sure you have frozen the correct layers
for i, layer in enumerate(vgg_model.layers):
    print(i, layer.name, layer.trainable)

x = vgg_model.output
x = Flatten()(x) # Flatten dimensions to for use in FC layers
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x) # Dropout layer to reduce overfitting
x = Dense(256, activation='relu')(x)
# logits = Dense(12, activation='sigmoid')(x)
x = Dense(1, activation='sigmoid')(x) # Softmax for multiclass
transfer_model = keras.Model(inputs=vgg_model.input, outputs=x)

learning_rate= 5e-5

transfer_model.compile(loss="binary_crossentropy",optimizer=optimizers.Adam(lr=learning_rate), metrics=["accuracy"])
history = transfer_model.fit(X_train, y_train, batch_size = 12, epochs=30)

X_test = []

for i in range(len(test_data)):
  for j in range(len(test_data[0])):
    X_test.append(test_data[i][j])

len(X_test)
X_test = np.array(X_test)

predictions = transfer_model.predict(X_test)

print(predictions)

y_pred = []

i = 0
z = 0

while i < len(predictions):
    mean = 0
    while z < 12:
        mean += predictions[i]
        z += 1
        i += 1
    z = 0
    mean = mean/12 
    if mean < 0.5:
        y_pred.append(0)
    else:
        y_pred.append(1)

print(y_pred)

print(len(y_pred))

print(type(y_test))

print(type(y_pred))

y_pred = np.array(y_pred)

y_test = np.argmax(y_test, axis=0)
y_pred = np.argmax(y_pred, axis=0)

print(y_test)

test = []
for i in range(len(y_test)):
    test.append(y_test[i])

y_test = np.array(test)