# -*- coding: utf-8 -*-
"""ML_ReportNotebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12WOo-26ovaqXv01m3tbWrI5GuvVwH2Kv

# HRV Functions
"""

!pip install neurokit2
!pip install hrv-analysis
!pip install pyhrv

import pandas as pd
import neurokit2 as nk  # This package can process ECG
import hrvanalysis as hrvana # RR interval processing package
import pandas as pd
import numpy as np
import pandas as pd
from matplotlib import style
import matplotlib.pyplot as plt
import hrvanalysis as hrvana # RR interval processing package
import numpy as np
import neurokit2 as nk  # This package can process ECG
import pyhrv.tools as tools
from pyhrv.hrv import hrv

from google.colab import drive
drive.mount('/content/drive')

def heart_rate_feature_extraction(data, sampling_rate = 500):

    plt.style.use('bmh')

    cleaned = nk.ecg_clean(data, sampling_rate=sampling_rate, method="pantompkins1985") # cleaning ECG 
    pantompkins1985 = nk.ecg_findpeaks(cleaned, method="pantompkins1985") # find the R peaks
    hrv_df = pd.DataFrame(pantompkins1985)

    hrv_df["RR Intervals"] = hrv_df["ECG_R_Peaks"].diff()
    hrv_df.loc[0, "RR Intervals"]=hrv_df.loc[0]['ECG_R_Peaks'] # the first datapoint contain Nan we manually fix it

    clean_rri = hrv_df['RR Intervals'].values/sampling_rate*1000 # Convert sample to ms
    clean_rri = hrvana.remove_outliers(rr_intervals=clean_rri, low_rri=300, high_rri=2000)
    clean_rri = hrvana.interpolate_nan_values(rr_intervals=clean_rri, interpolation_method="linear")
    clean_rri = hrvana.remove_ectopic_beats(rr_intervals=clean_rri, method="malik")
    clean_rri = hrvana.interpolate_nan_values(rr_intervals=clean_rri, interpolation_method="linear")

    hrv_df["RR Intervals"] = clean_rri 
    hrv_df["RR Intervals"].isna().any()

    nn_epoch = hrv_df['RR Intervals'].values

    feature_list = []
    all_hr_features = {}
    all_hr_features.update(hrvana.get_time_domain_features(nn_epoch))
    all_hr_features.update(hrvana.get_frequency_domain_features(nn_epoch))
    all_hr_features.update(hrvana.get_poincare_plot_features(nn_epoch))
    all_hr_features.update(hrvana.get_csi_cvi_features(nn_epoch))
    all_hr_features.update(hrvana.get_geometrical_features(nn_epoch))
    feature_list.append(all_hr_features)
    hrv_feature_df = pd.DataFrame(feature_list)
    hrv_feature_df.isna().any()
    
    return hrv_feature_df  #Return cleaned updated rri_interval in ms.

def return_hrv_participant_wise(data: pd.DataFrame(), sampling_rate = 500):
    hrv = pd.DataFrame()
    list_of_leads = ['II', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']
    for each_column in list_of_leads:
        try:
            df1 = heart_rate_feature_extraction(np.array(data[each_column]), sampling_rate = sampling_rate)
            df1.columns = each_column + "_"  + df1.columns
            hrv = pd.concat([hrv, df1], axis = 1)
            hrv.reset_index(inplace = True, drop =True)
        except:
            continue
    return hrv

"""# Making HRV Dataset"""

import pandas as pd

df = pd.read_excel("/content/drive/MyDrive/ecg_dataset/without DASS Satyam_data for DL analysis_Oct23_2022.xlsx")
df = df.loc[:,["x", "Based on Mni -Z, Q-3, Burn Out_V1", "Based on Mni -Z, Q-3, Burn Out_V2", "Based on Mni -Z, Q-3, Burn Out_V3", "Group Alloted"]]
df.rename(columns = {'x': 'ID', 'Based on Mni -Z, Q-3, Burn Out_V1':'V1', "Based on Mni -Z, Q-3, Burn Out_V2":"V2", "Based on Mni -Z, Q-3, Burn Out_V3":"V3", "Group Alloted":"Group"}, inplace = True)
df['V1'] = df['V1'].map({'Y': 1, 'N': 0})
df['V2'] = df['V2'].map({'Y': 1, 'N': 0})
df['V3'] = df['V3'].map({'Y': 1, 'N': 0})
df['V1_path'] = pd.Series(dtype=str)
df['V2_path'] = pd.Series(dtype=str)
df['V3_path'] = pd.Series(dtype=str)
df["Group"] = df["Group"].astype('int')
asd = []
for i in df['ID']:
    asd.append(int(str(i[2:])))
df["ID"] = asd
df = df.drop_duplicates(keep='first')
df = df.set_index('ID')

import openpyxl

wb = openpyxl.load_workbook("/content/drive/MyDrive/ecg_dataset/folder_data.xlsx")
ws = wb['Sheet1']
ID_data = []
for cell in ws["H"]:
    if cell.value==None: break
    try:
      ID_data.append((int(cell.value[2:])))
    except:
      continue

for i in ID_data:
    try:
        df.loc[i, 'V1_path'] = f"/content/drive/MyDrive/ecg_dataset/Visit 1/Group {df.loc[i, 'Group']}/{i} Rm/{i}_Rm.xls"
        ndf = pd.DataFrame(pd.read_excel(df.loc[i, 'V1_path']))
    except:
        df.loc[i, 'V1_path'] = f"/content/drive/MyDrive/ecg_dataset/Visit 1/Group {df.loc[i, 'Group']}/{i} Rm/{i}_Rmv.xls"
        ndf = pd.DataFrame(pd.read_excel(df.loc[i, 'V1_path']))
    try:
        df.loc[i, 'V2_path'] = f"/content/drive/MyDrive/ecg_dataset/Visit 2/Group {df.loc[i, 'Group']}/0{i} Rm/0{i}_Rm.xls"
        ndf = pd.DataFrame(pd.read_excel(df.loc[i, 'V2_path']))
    except:
        try:
            df.loc[i, 'V2_path'] = f"/content/drive/MyDrive/ecg_dataset/Visit 2/Group {df.loc[i, 'Group']}/0{i} Rm/00{i}_Rm.xls"
            ndf = pd.DataFrame(pd.read_excel(df.loc[i, 'V2_path']))
        except:
            df.loc[i, 'V2_path'] = f"/content/drive/MyDrive/ecg_dataset/Visit 2/Group {df.loc[i, 'Group']}/0{i} Rm/0{i}_Rmv.xls"
            ndf = pd.DataFrame(pd.read_excel(df.loc[i, 'V2_path']))
    try:
        df.loc[i, 'V3_path'] = f"/content/drive/MyDrive/ecg_dataset/Visit 3/Group {df.loc[i, 'Group']}/00{i} Rm/00{i}_Rm.xls"
        ndf = pd.DataFrame(pd.read_excel(df.loc[i, 'V3_path']))
    except:
        try:
            df.loc[i, 'V3_path'] = f"/content/drive/MyDrive/ecg_dataset/Visit 3/Group {df.loc[i, 'Group']}/00{i} Rm/000{i}_Rm.xls"
            ndf = pd.DataFrame(pd.read_excel(df.loc[i, 'V3_path']))
        except:
            df.loc[i, 'V3_path'] = f"/content/drive/MyDrive/ecg_dataset/Visit 3/Group {df.loc[i, 'Group']}/00{i} Rm/00{i}_Rmv.xls"
            ndf = pd.DataFrame(pd.read_excel(df.loc[i, 'V3_path']))
# print(df)
df.to_pickle('path_dataset.pkl')

class df_holder:
    def __init__(self, df): 
        self.df = df

for i in ID_data:
    df.loc[i, 'V1_df'] = df_holder(pd.read_excel(df.loc[i, 'V1_path']))
    df.loc[i, 'V2_df'] = df_holder(pd.read_excel(df.loc[i, 'V2_path']))
    df.loc[i, 'V3_df'] = df_holder(pd.read_excel(df.loc[i, 'V3_path']))

df.to_pickle('class_dataset.pkl')

df__ = pd.read_pickle("/content/drive/MyDrive/class_dataset.pkl")
df__.loc[1001,'V1_df'].df

dfs = ['V1_df', 'V2_df', 'V3_df']
hrv_dataset = df__
for ind in df__.index:
    for df_visit in dfs:
        ndf = df__.loc[ind, df_visit].df
        ndf = ndf.drop([0, 5007])
        ndf_hrv = return_hrv_participant_wise(ndf)
        hrv_dataset.loc[ind, df_visit+'HRV'] = df_holder(ndf_hrv)

hrv_dataset

hrv_dataset = hrv_dataset.drop(['V1_path',
 'V2_path',
 'V3_path',
 'V1_df',
 'V2_df',
 'V3_df'], axis = 1)
hrv_dataset.rename(columns = {'V1_dfHRV': 'V1_df', 'V2_dfHRV': 'V2_df', 'V3_dfHRV': 'V3_df'}, inplace = True)
lds = {"id":[],"v":[]}
for i in hrv_dataset.loc[1001, "V1_df"].df.columns.tolist():
    lds[i]=[]

for i in hrv_dataset.loc[1001, "V1_dfHRV"].df.columns.tolist():
    lds[i]=[]

n_ds = pd.DataFrame(lds)


for i in hrv_dataset.index:
    row = [i, hrv_dataset.loc[i, "V1"]]
    row.extend(hrv_dataset.loc[i, "V1_df"].df.values.tolist()[0])
    row.extend(hrv_dataset.loc[i, "V1_dfHRV"].df.values.tolist()[0])
    n_ds.loc[len(n_ds)] = row
    row = [i, hrv_dataset.loc[i, "V2"]]
    row.extend(hrv_dataset.loc[i, "V2_df"].df.values.tolist()[0])
    row.extend(hrv_dataset.loc[i, "V2_dfHRV"].df.values.tolist()[0])
    n_ds.loc[len(n_ds)] = row
    row = [i, hrv_dataset.loc[i, "V3"]]
    row.extend(hrv_dataset.loc[i, "V3_df"].df.values.tolist()[0])
    row.extend(hrv_dataset.loc[i, "V3_dfHRV"].df.values.tolist()[0])
    n_ds.loc[len(n_ds)] = row

n_ds.dropna(axis=1, how = 'all', inplace=True) # drop columns with all missing values
n_ds.dropna(axis=0, how = 'any', inplace=True) # drop rows with any missing values

n_ds

n_ds.to_pickle('complete_data.pkl')

"""# EDA"""

class df_holder:
    def __init__(self, df): 
        self.df = df

df__ = pd.read_pickle("/content/drive/MyDrive/class_dataset.pkl")
df__.loc[1001,'V1_df'].df

_lr,_lc = 3,4
_leads = [
    [['I'],['avR'],['V1'],['V4']],
    [['II'],['avL'],['V2'],['V5']],
    [['III'],['avF'],['V3'],['V6']]
]

df__1001 = df__.loc[1001,'V1_df'].df


fig,ax = plt.subplots(_lr,_lc, figsize=(_lr*7,_lc*4))
#fig.set_dpi(150)
for i in range(0,3):
    for j in range(0,4):
        _key = _leads[i][j][0]
        ax[i,j].set_title(_key)
        ax[i,j].plot(df__1001[_key], color='black', linewidth=0.6)
        ax[i,j].set_ylim((-1500,1500))
        ax[i,j].set_xlim((0,5000))
        ax[i,j].hlines(0,0,5000,color='black',linestyle='dotted')


plt.show()

from scipy import signal

df = df__.loc[1001,'V1_df'].df

fig,ax = plt.subplots(_lr,_lc, figsize=(_lr*7,_lc*4))
for i in range(0,3):
    for j in range(0,4):
        _key = _leads[i][j][0]
        ax[i,j].set_title(_key)
        sos = signal.butter(5, 0.3, 'hp', fs=2000, output='sos')
        sig = df[_key]
        sig = sig.drop([0, 5007])
        clean_sig = nk.ecg_clean(sig, 500)
        filtd=signal.sosfilt(sos, sig)
        ax[i,j].plot(sig, label='Raw Signal')
        ax[i,j].plot(clean_sig, label='Cleaned Signal')
        ax[i,j].legend(['Raw Signal', 'Cleaned Signal'])

plt.show()

df = pd.read_pickle('/content/drive/MyDrive/ecg_dataset/clustered_data.pkl')
df = df.rename(columns={'v': 'target'})
df

target_values = df['target'].values
label_values, label_counts = np.unique(target_values, return_counts=True)
x_labels = ['Not stressed', 'Burn out/Stressed']
## burnt out == 1
# Plot a bar chart of the label frequencies using matplotlib
plt.bar(x_labels, label_counts, tick_label=x_labels)
plt.xlabel('Mental state')
plt.ylabel('Count')
plt.title('Class Distribution')
plt.show()

import matplotlib.pyplot as plt

# assume y is your label data
classes, counts = np.unique(target_values, return_counts=True)

# plot the class distribution as a pie chart
plt.pie(counts, labels=['Not stressed', 'Burn out/Stressed'], autopct='%1.1f%%')
plt.title('Class Distribution')
plt.show()

df.describe()

"""# Data Handling"""

import pandas as pd
df = pd.read_pickle('/content/drive/MyDrive/ecg_dataset/complete_data.pkl')
df = df.rename(columns={'v': 'target'})

class_counts = df['target'].value_counts()
print(class_counts)

X, y = df.drop(['target', 'id'], axis=1), df['target']

X.shape

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

from sklearn.decomposition import PCA

# Create a PCA object with desired number of components
pca = PCA(0.95)

# Fit and transform the data using PCA
X_red = pca.fit_transform(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_red, y, test_size=0.33, random_state=42,stratify = y)

y_train.value_counts()

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

over = SMOTE(sampling_strategy=0.8)
X_resampled, y_resampled = over.fit_resample(X_train, y_train)

y_resampled.value_counts()

import numpy as np
unique_elements, counts = np.unique(y_resampled, return_counts=True)
print(counts)

X_train = X_resampled 
y_train = y_resampled

"""# Clustering"""

import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_pickle('/content/drive/MyDrive/ecg_dataset/complete_data.pkl')
df = df.rename(columns={'v': 'target'})
df

cls_data = df.loc[:, 'II_mean_nni':'V6_triangular_index']
cls_data

# standardizing the data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# data= X_new
data_scaled = scaler.fit_transform(cls_data)

# statistics of scaled data
pd.DataFrame(data_scaled).describe()

from sklearn.decomposition import PCA
pca = PCA(n_components = 1)
pC = pca.fit_transform(data_scaled)
principalDf = pd.DataFrame(data = pC, columns = ['feature1'])
principalDf

# k means using 2 clusters and k-means++ initialization
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters = 3, init='k-means++')
kmeans.fit(principalDf)
pred = kmeans.predict(principalDf)
frame = pd.DataFrame(principalDf)
frame['cluster'] = pred
frame['target'] = df['target'].astype(int)
frame
print(frame['target'].value_counts())
frame['cluster'].value_counts()

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 1', fontsize = 15)
ax.set_title('PCA', fontsize = 20)

colors = ['r', 'g', 'b']
for i in frame.index:
    ax.scatter(frame.loc[i, 'feature1'],
               frame.loc[i, 'feature2']
                , c = colors[frame.loc[i,'cluster']]
                )
ax.grid()

print(frame['target'].value_counts())
frame['cluster'].value_counts()

frame['cluster'].replace(0, 3, inplace=True)
frame['cluster'].replace(2, 0, inplace=True)
frame['cluster'].replace(3, 2, inplace=True)

from sklearn.metrics import accuracy_score, silhouette_score
print('Accuracy:', accuracy_score(frame['target'], frame['cluster']))
print('Silhoutte score:', silhouette_score(frame, frame['cluster']))

frame = frame[frame['cluster'] != 2]
frame

new_df = df[df.index.isin(frame.index)]
new_df

new_df['target'] = frame['cluster']
new_df

new_df

new_df.to_pickle('clustered_data.pkl')

"""# Classification"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

accuracies = []
k_values = range(1, 31)
for i in k_values:
    knn_clf = KNeighborsClassifier(n_neighbors=i)
    knn_clf.fit(X_train, y_train)
    # knn_clf_pred = knn_clf.predict(X_test)
    accuracy = knn_clf.score(X_test, y_test)
    accuracies.append(f'{accuracy:.2f}')
accuracy = max(accuracies)
k = accuracies.index(max(accuracies))+1
print(f'Max Accuracy: {accuracy}')
plt.plot(k_values, accuracies)
plt.xlabel('k')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. k for k-NN')
plt.show()
print(k)

knn_clf = KNeighborsClassifier(n_neighbors=k)
knn_clf.fit(X_train, y_train)
y_pred = knn_clf.predict(X_test)

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Train and evaluate a decision tree model
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
# print(y_pred_dt)
accuracy_dt = accuracy_score(y_test, y_pred_dt)

# Train and evaluate a random forest model
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
# print(y_pred_rf)
accuracy_rf = accuracy_score(y_test, y_pred_rf)


# Print the accuracies of each model
print(f'Decision tree accuracy: {accuracy_dt:.2f}')
print(f'Random forest accuracy: {accuracy_rf:.2f}')

from sklearn.model_selection import GridSearchCV

# Define the hyperparameters grid
param_grid = {
    'n_neighbors': range(1, 21),
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski']
}

# Create a k-NN classifier object
knn = KNeighborsClassifier()

# Perform a grid search to find the best hyperparameters
grid_search = GridSearchCV(knn, param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Print the best hyperparameters and the corresponding accuracy on the testing set
print(f'Best hyperparameters: {grid_search.best_params_}')
print(f'Testing accuracy: {grid_search.score(X_test, y_test):.2f}')

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf1 = RandomForestClassifier(n_estimators = 100,criterion='gini',min_samples_leaf=10,min_samples_split=20,class_weight='balanced') 
rf1.fit(X_train,y_train)
print("ACCURACY OF THE MODEL: ", round(accuracy_score(y_test, rf1.predict(X_test)),2))

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# try
kernels = ['poly', 'rbf', 'sigmoid']

kernel = 'poly'
svm = SVC(kernel=kernel)
svm.fit(X_train, y_train)
y_pred = svm.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'SVM accuracy with {kernel} kernel: {accuracy:.2f}')

from sklearn.naive_bayes import GaussianNB  
classifier = GaussianNB()  
classifier.fit(X_train, y_train) 
y_pred = classifier.predict(X_test)  
accuracy = accuracy_score(y_test, y_pred)
from sklearn.metrics import confusion_matrix  
cm = confusion_matrix(y_test, y_pred)
print(accuracy)
print(cm)

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

# create a Random Forest Classifier object
rfc = SVC(kernel='rbf')

# create a 5-fold stratified cross-validator
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# initialize lists to store the evaluation metrics for each fold
acc_list = []
f1_list = []
precision_list = []
recall_list = []
auc_list = []

# loop over the folds and train/test the classifier
for fold, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):
    # get the train and test data for this fold
    
    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]
    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]

    # fit the classifier on the train data
    rfc.fit(X_train_fold, y_train_fold)

    # predict the labels on the test data
    y_pred = rfc.predict(X_test_fold)

    # compute the evaluation metrics
    acc = accuracy_score(y_test_fold, y_pred)
    f1 = f1_score(y_test_fold, y_pred)
    precision = precision_score(y_test_fold, y_pred)
    recall = recall_score(y_test_fold, y_pred)
    auc = roc_auc_score(y_test_fold, y_pred)

    # append the evaluation metrics to the lists
    acc_list.append(acc)
    f1_list.append(f1)
    precision_list.append(precision)
    recall_list.append(recall)
    auc_list.append(auc)

    # print the evaluation metrics for this fold
    print(f"Fold {fold + 1}: Accuracy = {acc:.3f}, F1 Score = {f1:.3f}, Precision = {precision:.3f}, Recall = {recall:.3f}, AUC = {auc:.3f}")

# print the mean evaluation metrics across all folds
print(f"\nAcross folds: Accuracy = {np.mean(acc_list):.3f}, F1 Score = {np.mean(f1_list):.3f}, Precision = {np.mean(precision_list):.3f}, Recall = {np.mean(recall_list):.3f}, AUC = {np.mean(auc_list):.3f}")

"""## Evaluation Metrics"""



##Metrics
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score
y_true = y_test
cm = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(cm)

# compute classification report with precision, recall, and F1 score
print("\nClassification Report:")
print(classification_report(y_true, y_pred))

#compute accuracy
acc = accuracy_score(y_pred, y_true)
print("Accuracy: ", acc)
# compute F1 score
f1 = f1_score(y_true, y_pred)
print(f"F1 Score: {f1}")

# compute precision
precision = precision_score(y_true, y_pred)
print(f"Precision: {precision}")

# compute recall
recall = recall_score(y_true, y_pred)
print(f"Recall: {recall}")

# compute AUC
auc = roc_auc_score(y_true, y_pred)
print(f"AUC: {auc}")

"""# Using spectogram data"""

import pandas as pd
import scipy.signal
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL as pil
import numpy as np
import cv2
import sys
from keras.layers import Flatten
from keras.layers import Dense
from keras.layers import Dropout
from keras import optimizers
from tensorflow import keras
import PIL as pil
import multiprocessing as mp
import pickle
from keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
from tensorflow.keras.applications.vgg16 import VGG16
import tensorflow as tf

from google.colab import drive
drive.mount('/content/drive')

class df_holder:
    def __init__(self, df): 
        self.df = df

lead_data = pd.read_pickle("/content/drive/MyDrive/ecg_dataset/class_dataset.pkl")

train_1 = lead_data.loc[:, ['V1', 'V1_df', 'V2', 'V2_df']]
test = lead_data.loc[:, ['V3', 'V3_df']]
a_df = pd.concat([train_1['V1'], train_1['V2']])
b_df = pd.concat([train_1['V1_df'], train_1['V2_df']])

# create new dataframe with concatenated columns
train = pd.DataFrame({'target': a_df.values, 'df_holder': b_df.values})
train
test = test.rename(columns={'V3': 'target', 'V3_df': 'df_holder'})
test
X_train_, y_train = train['df_holder'].values, train['target'].values
X_test_, y_test = test['df_holder'].values, test['target'].values

import numpy as np
X_test = np.zeros((202, 5000, 12))
X_train = np.zeros((404, 5000, 12))
print(X_test.shape)
for i in range(len(X_test)):
    X_test[i] = X_test_[i].df[1:5001].values.astype(float)
for i in range(len(X_train)):
    X_train[i] = X_train_[i].df[1:5001].values.astype(float)

print(train['target'].value_counts())

# Separating classes
from sklearn.utils import resample
burntout = train[train['target'].values == 1]
not_burntout = train[train['target'].values == 0]

undersample = resample(burntout, 
                       replace=True, 
                       n_samples=len(not_burntout), #set the number of samples to equal the number of the minority class
                       random_state=42)
# Returning to new training set
train_balanced = pd.concat([not_burntout, undersample])

print(train_balanced['target'].value_counts())

X_train_, y_train = train_balanced['df_holder'].values, train_balanced['target'].values

import numpy as np
X_test = np.zeros((202, 5000, 12))
X_train = np.zeros((404, 5000, 12))
print(X_test.shape)
for i in range(len(X_test)):
    X_test[i] = X_test_[i].df[1:5001].values.astype(float)
for i in range(len(X_train)):
    X_train[i] = X_train_[i].df[1:5001].values.astype(float)

# os.mkdir('drive/MyDrive/Spectrograms/train')
import numpy as np
from scipy.signal import spectrogram

X_train_spec = []
y_train_spec = []
for j in range(len(X_train)):
  # os.mkdir(f'drive/MyDrive/Spectrograms/train/Participant{j+1}')
  for i in range(12):
    data = pd.DataFrame(X_train[j])
    data = data.iloc[:,i]
    # plt.figure()

    fs = 500

    f, t, Sxx = spectrogram(data, fs=fs)
    spectrogram_array = np.array(Sxx)

    X_train_spec.append(spectrogram_array)
    y_train_spec.append(y_train[j])
    # plt.show()
    # plt.savefig(f'drive/MyDrive/Spectrograms/train/Participant{j+1}/lead{i+1}.png')

X_train_spec = np.array(X_train_spec)
y_train_spec = np.array(y_train_spec)

with open('/content/drive/MyDrive/ecg_dataset/spectogram_train_X', 'wb') as f:
  pickle.dump(X_train_spec,f)
with open('/content/drive/MyDrive/ecg_dataset/spectogram_train_y', 'wb') as f:
  pickle.dump(y_train_spec,f)

# os.mkdir('drive/MyDrive/Spectrograms/train')
import numpy as np
from scipy.signal import spectrogram

X_test_spec = []
y_test_spec = []
for j in range(len(X_test)):
  # os.mkdir(f'drive/MyDrive/Spectrograms/train/Participant{j+1}')
  for i in range(12):
    data = pd.DataFrame(X_test[j])
    data = data.iloc[:,i]
    # plt.figure()
    fs = 500

    f, t, Sxx = spectrogram(data, fs=fs)
    spectrogram_array = np.array(Sxx)

    X_test_spec.append(spectrogram_array)
    y_test_spec.append(y_test[j])
    # plt.show()
    # plt.savefig(f'drive/MyDrive/Spectrograms/train/Participant{j+1}/lead{i+1}.png')

X_test_spec = np.array(X_test_spec)
y_test_spec = np.array(y_test_spec)

with open('/content/drive/MyDrive/ecg_dataset/spectogram_test_X', 'wb') as f:
  pickle.dump(X_test_spec,f)
with open('/content/drive/MyDrive/ecg_dataset/spectogram_test_y', 'wb') as f:
  pickle.dump(y_test_spec,f)

train_path = os.listdir("/content/drive/MyDrive/Spectrograms/train")
test_path = os.listdir("/content/drive/MyDrive/Spectrograms/test")

def process_image(path):
  img = pil.Image.open(path)
  img = img.resize((224,224))
  img = data = np.asarray(img)
  return data

import cv2
from google.colab.patches import cv2_imshow

from PIL import Image
train_data = []
for i in train_path:
  print(i)
  participant_path = os.listdir("/content/drive/MyDrive/Spectrograms/train/"+i)
  input_vector = []
  #with mp.Pool(processes=8) as pool:
  input_paths = [f"/content/drive/MyDrive/Spectrograms/train/{i}/{j}" for j in participant_path]
#   input_vector = pool.map(process_image, input_paths[0])
  img = cv2.imread(input_paths[0])
  img_array = np.array(img)
  cv2_imshow(img)
  pl = Image.open(input_paths[0])
  input_vector = np.asarray(pl)
  print(img_array[:,:,0][0])
  print(input_vector)
  break
  array = np.array(input_vector)
  train_data.append(array)

train_data = np.array(train_data)

from PIL import Image
test_data = []
test_label = []
for i in test_path:
  print(i)
  participant_path = os.listdir("/content/drive/MyDrive/Spectrograms/test/"+i)
  input_vector = []
  input_paths = [f"/content/drive/MyDrive/Spectrograms/test/{i}/{j}" for j in participant_path]
  for image in input_paths:
    input_vector = np.asarray(Image.open(image))   

    array = np.array(input_vector)
    test_data.append(array)
    test_label.append(y_test[i])
test_data = np.array(test_data)
test_label = np.array(test_label)

file_name_test = "/content/drive/MyDrive/Spectrograms/test_set"
file_name_train = "/content/drive/MyDrive/Spectrograms/train_set"

train_set = train_data

with open(file_name_train, 'wb') as f:
  pickle.dump(train_data,f)

test_set = test_data

with open(file_name_test, 'wb') as f:
  pickle.dump(test_data,f)

new_y_train = []
for i in range(len(y_train)):
  temp = [y_train[i]]*12
  new_y_train.append(temp)

y_train = np.array(new_y_train)

len(y_train)
y_train = y_train.flatten()
len(y_train)

new_y_test = []
for i in range(len(y_test)):
  temp = [y_test[i]]*12
  new_y_test.append(temp)

y_test = np.array(new_y_test)

with open('/content/drive/MyDrive/Spectrograms/train_set', 'rb') as f:
    train_data = pickle.load(f)

with open('/content/drive/MyDrive/Spectrograms/test_set', 'rb') as f:
    test_data = pickle.load(f)

train_data.shape

X_train = []

for i in range(len(train_data)):
  for j in range(len(train_data[0])):
    X_train.append(train_data[i][j])

len(X_train)
X_train = np.array(X_train)
# X_test, y_test = test_set

vgg_model = VGG16(weights='imagenet',include_top=False, input_shape=(224, 224, 3))

input_shape = (12, 224, 224, 3)
input_tensor = Input(shape=input_shape)

# Reshape input tensor to have the correct shape
# x = tf.reshape(input_tensor, [-1, input_shape[1], input_shape[2], input_shape[3]])


input_tensor = Input(shape=(224, 224, 3))
vgg_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)

# Add your own layers on top of the VGG16 model here
# ...

# model = Model(inputs=input_tensor, outputs=...)

for layer in vgg_model.layers:
  layer.trainable = False
# Make sure you have frozen the correct layers
for i, layer in enumerate(vgg_model.layers):
    print(i, layer.name, layer.trainable)

x = vgg_model.output
x = Flatten()(x) # Flatten dimensions to for use in FC layers
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x) # Dropout layer to reduce overfitting
x = Dense(256, activation='relu')(x)
# logits = Dense(12, activation='sigmoid')(x)
x = Dense(1, activation='sigmoid')(x) # Softmax for multiclass
transfer_model = keras.Model(inputs=vgg_model.input, outputs=x)

X_train = np.zeros((4848, 224, 224, 3))
X_test = np.zeros((2424, 224, 224, 3))
for i in range(4848):
    # resize each image to (224, 224) and convert to RGB format
    img = X_train_spec[i]
    img = cv2.resize(img, (224, 224))
    X_train[:, :, :, 0] = img
    X_train[:, :, :, 1] = img
    X_train[:, :, :, 2] = img
for i in range(2424):
    # resize each image to (224, 224) and convert to RGB format
    img = X_test_spec[i]
    img = cv2.resize(img, (224, 224))
    X_test[:, :, :, 0] = img
    X_test[:, :, :, 1] = img
    X_test[:, :, :, 2] = img

learning_rate= 5e-5

transfer_model.compile(loss="binary_crossentropy",optimizer=optimizers.Adam(lr=learning_rate), metrics=["accuracy"])
history = transfer_model.fit(X_train_spec, y_train_spec, batch_size = 12, epochs=30, validation_data=(X_test_spec, y_test_spec))

X_test = []

for i in range(len(test_data)):
  for j in range(len(test_data[0])):
    X_test.append(test_data[i][j])

len(X_test)
X_test = np.array(X_test)

predictions = transfer_model.predict(X_test)

y_pred = []

i = 0
z = 0

while i < len(predictions):
    mean = 0
    while z < 12:
        mean += predictions[i]
        z += 1
        i += 1
    z = 0
    mean = mean/12 
    if mean < 0.5:
        y_pred.append(0)
    else:
        y_pred.append(1)

print(y_pred)

y_test = np.argmax(y_test, axis=0)
y_pred = np.argmax(y_pred, axis=0)

import matplotlib.pyplot as plt
import numpy as np

# Define your data in a table format
models = ['K-nearest neighbors(k=2)',
'Decision Tree',
'Random forest',
'RBF kernel SVM',
'Naive bayes',
]
metrics = ['Accuracy', 'AUC','F1 Score', 'Precision', 'Recall']

# [0.705,0.549,0.289,0.333,0.255],
# [0.645,0.517,0.268,0.26,0.276],
# [0.76, 0.533, 0.518,0.428,0.063],
# [0.72, 0.982,0.522,0.304,0.148],
# [0.6,0.554,0.554,0.285,0.468]

[0.97,0.97,0.971,0.944,1.0],
[0.98,0.982,0.98,1.0,0.96],
[0.982,0.982,0.982,1.0,0.964],
[0.982,0.982,0.982,0.976,0.988],
[0.877,0.956,0.877,0.847,0.917]





scores = np.array([
[0.97,0.97,0.971,0.944,1.0],
[0.98,0.982,0.98,1.0,0.96],
[0.982,0.982,0.982,1.0,0.964],
[0.982,0.982,0.982,0.976,0.988],
[0.877,0.956,0.877,0.847,0.917]])

# Set up the bar graph
bar_width = 0.1
x_pos = np.arange(len(models))

fig, ax = plt.subplots(figsize=(10, 6))  # Increase the figsize


# Plot the bars for each metric and model
for i, metric in enumerate(metrics):
    plt.bar(x_pos + i*bar_width, scores[:, i], width=bar_width, label=metric)

ax.set_ylim(0.825, 1.005)  # Set the y-axis limits


# Add labels and legends
plt.xticks(x_pos + bar_width, models)
plt.title('Evaluation Metrics by Model')
plt.xlabel('Models')
plt.ylabel('Scores')
plt.legend()

# Show the graph
plt.show()